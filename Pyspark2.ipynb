{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ab6499-c96e-45be-8060-ee9f8d256d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo spark')\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59951af6-141b-4651-a9f7-d7c70ad03b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e12037e-2642-496d-802b-df6f8db485b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72f9bd95-d684-48bb-a0ac-c96927f51949",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = spark.sparkContext.parallelize([4, 5, 6, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c838210a-4e45-4df8-babc-562eeae08da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union(other)\n",
    "# A union will get all the elements from both the data sets.\n",
    "\n",
    "rdd_union =rdd1.union(rdd2)\n",
    "rdd_union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce47cc9-fcd5-46cd-85c0-f613143c2f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.union(rdd2).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7faf16df-bbd5-4ead-ba52-b8d1ee962450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54b317c9-1ae1-4432-a62d-4e7f3f4c498e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.intersection(rdd2).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec5d8f2-af14-41ba-863e-42d098f6d5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f281e6-5001-468f-b1b6-96ea199d43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pattern to remember\n",
    "rdd1 = spark.sparkContext.parallelize([\"a\", \"b\", \"c\", \"d\"])\n",
    "rdd2 = spark.sparkContext.parallelize([\"c\", \"d\", \"e\", \"f\"])\n",
    "\n",
    "rdd_intersect = rdd1.intersection(rdd2)\n",
    "print(rdd_intersect.collect())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a64f1-0c5d-4f59-b34c-c416675f6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4bcd0fb-f10a-43d7-b7ae-390795345e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of customers placed order in July or Aug Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffe74986-ca75-4df8-833c-cee850b4d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord = spark.sparkContext.textFile('c:/data/Orders')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98ca04d1-4209-457d-b780-f77708b6acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "julyOrd = ord.filter(lambda x : str(x.split(',')[1].split('-')[1]) == '07').map(lambda x : x.split(',')[2])\n",
    "augOrd = ord.filter(lambda x : str(x.split(',')[1].split('-')[1]) == '08').map(lambda x : x.split(',')[2])\n",
    "julyAugOrders = julyOrd.union(augOrd).distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43ea6610-96d4-4c1c-a87e-1a06eca594b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7633\n"
     ]
    }
   ],
   "source": [
    "print(julyAugOrders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7daa6ef5-3c67-469e-8c40-769c42daacf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3836"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Orders applied both in July and Aug datamonth.\n",
    "julyAugCommonOrders=julyOrd.subtract(augOrd).count()\n",
    "julyAugCommonOrders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97d91f03-dd7d-40c3-8b56-b1ca2bb31829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 25, 32, 35, 37, 38, 41, 44, 45, 56, 97]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample\n",
    "rdd = spark.sparkContext.parallelize(range(100), 4)\n",
    "rdd.sample(fraction=0.1,withReplacement=False).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68176f97-836f-4d43-805e-75319c1ed874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 37, 17, 69, 86]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take sample\n",
    "rdd.takeSample(seed=10,num=5,withReplacement=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64901741-eb70-49ff-bc33-53cab0936472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34564, 34319]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repartition\n",
    "ord = spark.sparkContext.textFile('c:/data/orders')\n",
    "ord.glom().map(len).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "267b2cf6-f592-4019-b6ef-f941760f8b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13783, 13770, 13770, 13770, 13790]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord1 =ord.repartition(5)\n",
    "ord1.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4902cd0f-f98b-4b35-8a87-27e4f0d93dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord2 = ord1.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a311160e-6307-42f7-ad2e-d43bb2e72170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f012c2c-8330-4708-bee9-437640853a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17414, 17150, 17165, 17154]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord = spark.sparkContext.textFile('c:/data/orders',4)\n",
    "ord.glom().map(len).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93a074ec-8477-40d5-8d4a-50ba8bb3a11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce(5)\n",
    "ord = spark.sparkContext.textFile('c:/data/orders',4)\n",
    "ord.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e87c34bb-63a0-488d-bf9f-bdf8e63611c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord1.coalesce(1).getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3871f2d8-c2ef-4dda-8238-1c0c71c84b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveAsTextFile\n",
    "julyOrd = ord.filter(lambda x : str(x.split(',')[1].split('-')[1]) == '07')\n",
    "augOrd = ord.filter(lambda x : str(x.split(',')[1].split('-')[1]) == '08')\n",
    "julyAugOrders = julyOrd.union(augOrd).distinct()\n",
    "julyAugOrders.coalesce(1).saveAsTextFile('c:/data/part3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "392161ab-d9c2-46c2-8fcb-7d747b39f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([('a','apple'),('b','ball'),('c','cat')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6fc30cd-3d1d-480d-8512-05c7de9fa570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[104] at readRDDFromFile at PythonRDD.scala:297"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c4d1f3b-67ad-4be4-818f-057c0d37a264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "924c918d-c206-4ae7-a1b4-5d606dd4d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsSequenceFile('c:/data/seq1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f15e2143-bfd5-4d6e-b73f-812a0b928e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd persistent\n",
    "rdd = spark.sparkContext.parallelize((\"b\", \"a\", \"c\"))\n",
    "print(rdd.persist().is_cached)\n",
    "rdd.getStorageLevel()  #default memory_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7faa162e-13d1-4e75-96db-062939c4aa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, False, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "rdd.unpersist()\n",
    "\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK_2) #ore RDD partitions in memory; if memory is not enough, spill extra partitions to disk\n",
    "                                            #_2\tReplicate each partition 2 times across different nodes (for fault tolerance)\n",
    "\n",
    "rdd.getStorageLevel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2e7d64-dbbc-4e38-9ef3-3995f3da0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A broadcast variable is a read-only distributed variable cached on each executor, \n",
    "# used to speed up operations by avoiding repeated data transfer.\n",
    "# When to Use?\n",
    "# Use broadcast when:\n",
    "#     You have a small lookup table (e.g., 10 MB)\n",
    "#     You join large RDD with small RDD\n",
    "#     You use common config rules, mapping dictionaries, tax slabs, country-codes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f690e1a1-2ff6-4af5-bcfb-f668030d83fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 'Ravi', 'HR'), (102, 'Kiran', 'IT'), (103, 'John', 'Finance')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small lookup table\n",
    "dept_lookup = {1: \"HR\", 2: \"Finance\", 3: \"IT\"}\n",
    "broadcast_dept = spark.sparkContext.broadcast(dept_lookup)\n",
    "\n",
    "emp_rdd = spark.sparkContext.parallelize([\n",
    "    (101, \"Ravi\", 1),\n",
    "    (102, \"Kiran\", 3),\n",
    "    (103, \"John\", 2)\n",
    "])\n",
    "\n",
    "result = emp_rdd.map(\n",
    "    lambda x: (x[0], x[1], broadcast_dept.value[x[2]])\n",
    ")\n",
    "\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88f6eeac-a29a-40fb-9ce5-b568638561e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1, 'HR'), (2, 'Finance'), (3, 'IT')])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_lookup.values()\n",
    "dept_lookup.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b68904a7-cde6-401c-a7ce-ceda055e774f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ravi', 50000, 5000.0), ('Kiran', 60000, 12000.0), ('John', 90000, 27000.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tax_rules = {  \n",
    "    \"A\": 10,   \n",
    "    \"B\": 20,   \n",
    "    \"C\": 30    \n",
    "}\n",
    "\n",
    "salary_rdd = spark.sparkContext.parallelize([\n",
    "    (\"Ravi\", \"A\", 50000),\n",
    "    (\"Kiran\", \"B\", 60000),\n",
    "    (\"John\", \"C\", 90000)\n",
    "])\n",
    "\n",
    "broadcast_tax = spark.sparkContext.broadcast(tax_rules)\n",
    "tax_rdd = salary_rdd.map(\n",
    "    lambda x: (x[0],x[2], x[2] * broadcast_tax.value[x[1]] / 100)\n",
    ")\n",
    "\n",
    "tax_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f20a1a3-1718-45ae-a1f2-746b59f16db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Accumulator is a write-only shared variable used for aggregating values \n",
    "        across tasks in a distributed cluster.\n",
    "#     Think of it like a global counter or global sum, where:\n",
    "#     Executors (workers) can only add values\n",
    "#     Driver can read the final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6cd5c3f-123a-43a9-9aca-a23d55dc98e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blank lines: 2\n"
     ]
    }
   ],
   "source": [
    "blank_lines = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def check_blank(line):\n",
    "    global blank_lines\n",
    "    if line.strip() == \"\":\n",
    "        blank_lines += 1\n",
    "    return line\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\"hello\", \"\", \"world\", \"  \", \"spark\"])\n",
    "\n",
    "rdd.map(check_blank).collect()\n",
    "\n",
    "print(\"Total blank lines:\", blank_lines.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33ad3375-cd0a-41e7-b298-0729591cbd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum: 100\n"
     ]
    }
   ],
   "source": [
    "sum_acc = spark.sparkContext.accumulator(0)\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([10, 20, 30, 40])\n",
    "\n",
    "def add_value(x, acc):\n",
    "    acc.add(x)\n",
    "\n",
    "rdd.foreach(lambda x: add_value(x, sum_acc))\n",
    "\n",
    "print(\"Total Sum:\", sum_acc.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cef47e-dbc6-4818-a41c-76cc9a994ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
