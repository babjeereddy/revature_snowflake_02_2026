{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250f5582-cfeb-4c5c-aaac-f5de7d06befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Spark Demo\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19d0836-313a-4a50-9934-c8bfd4de00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcad3e57-30f2-4c0c-aa81-f68491e40be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|emp_id| ename|dept_id|\n",
      "+------+------+-------+\n",
      "|     1| Smith|     10|\n",
      "|     2| Allen|     20|\n",
      "|     3|  Ward|     10|\n",
      "|     4| Jones|     30|\n",
      "|     5|Martin|   NULL|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# EMP schema\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"ename\", StringType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp_data = [\n",
    "    (1, \"Smith\", 10),\n",
    "    (2, \"Allen\", 20),\n",
    "    (3, \"Ward\", 10),\n",
    "    (4, \"Jones\", 30),\n",
    "    (5, \"Martin\", None)\n",
    "]\n",
    "\n",
    "emp = spark.createDataFrame(emp_data, emp_schema)\n",
    "\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe52e23d-834e-41c5-8dc4-bbc86f8cca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+------------+\n",
      "| ename|Name_upper|Name_lower|Name_initcap|\n",
      "+------+----------+----------+------------+\n",
      "| Smith|     SMITH|     smith|       Smith|\n",
      "| Allen|     ALLEN|     allen|       Allen|\n",
      "|  Ward|      WARD|      ward|        Ward|\n",
      "| Jones|     JONES|     jones|       Jones|\n",
      "|Martin|    MARTIN|    martin|      Martin|\n",
      "+------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(\"ename\",\n",
    "          upper(\"ename\").alias(\"Name_upper\"),\n",
    "          lower(\"ename\").alias(\"Name_lower\"),\n",
    "          initcap(\"ename\").alias(\"Name_initcap\")\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa658c66-1309-4136-beaf-c426f25088fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+\n",
      "|emp_id| ename|dept_id|Name_upper|\n",
      "+------+------+-------+----------+\n",
      "|     1| Smith|     10|     SMITH|\n",
      "|     2| Allen|     20|     ALLEN|\n",
      "|     3|  Ward|     10|      WARD|\n",
      "|     4| Jones|     30|     JONES|\n",
      "|     5|Martin|   NULL|    MARTIN|\n",
      "+------+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.withColumn(\"Name_upper\",upper(\"ename\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f54484-2b21-4e5d-9ba9-2fbd829d7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+----------+----------+------------+\n",
      "|emp_id| ename|dept_id|Name_upper|Name_lower|Name_initcap|\n",
      "+------+------+-------+----------+----------+------------+\n",
      "|     1| Smith|     10|     SMITH|     smith|       Smith|\n",
      "|     2| Allen|     20|     ALLEN|     allen|       Allen|\n",
      "|     3|  Ward|     10|      WARD|      ward|        Ward|\n",
      "|     4| Jones|     30|     JONES|     jones|       Jones|\n",
      "|     5|Martin|   NULL|    MARTIN|    martin|      Martin|\n",
      "+------+------+-------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.withColumns({\n",
    "    \"Name_upper\":upper(\"ename\"),\n",
    "    \"Name_lower\":lower(\"ename\"),\n",
    "    \"Name_initcap\":initcap(\"ename\")}\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4acf0c91-c641-40db-89cb-cb0a10942113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+\n",
      "| id|order_date|        delivery_ts|\n",
      "+---+----------+-------------------+\n",
      "|  1|2025-05-19|2025-12-15 14:25:00|\n",
      "|  2|2024-05-20|2025-11-01 09:10:30|\n",
      "|  3|2024-05-21|2025-07-30 20:45:10|\n",
      "+---+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b4eb67b-c9e1-4972-8bd0-8b6cb4758457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  1| Ravi Kumar|\n",
      "|  2|Priya Reddy|\n",
      "|  3|   John Doe|\n",
      "|  4|Kiran Kumar|\n",
      "|  5|Meera Priya|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Ravi Kumar\"),\n",
    "    (2, \"Priya Reddy\"),\n",
    "    (3, \"John Doe\"),\n",
    "    (4, \"Kiran Kumar\"),\n",
    "    (5, \"Meera Priya\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c0cba9f-f484-4f52-8219-501394717d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  1| Ravi Kumar|\n",
      "|  2|Priya Reddy|\n",
      "|  3|   John Doe|\n",
      "|  4|Kiran Kumar|\n",
      "|  5|Meera Priya|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7b37ae2-c6b7-469d-8fda-e897cfbc866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"upper_name\", lower(\"name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f3162eb-2757-48e1-a01a-2182adea1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "| id|       name| upper_name|\n",
      "+---+-----------+-----------+\n",
      "|  1| Ravi Kumar| ravi kumar|\n",
      "|  2|Priya Reddy|priya reddy|\n",
      "|  3|   John Doe|   john doe|\n",
      "|  4|Kiran Kumar|kiran kumar|\n",
      "|  5|Meera Priya|meera priya|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b82236c7-7e37-4905-bae9-84c7d0a4ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|              name|\n",
      "+---+------------------+\n",
      "|  1|    Ravi Kumar    |\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(trim(col(\"name\")) == 'Ravi Kumar').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58a6218-8e9b-49d6-b2f3-e255c035ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "| id|       name| upper_name|\n",
      "+---+-----------+-----------+\n",
      "|  1| Ravi Kumar| RAVI KUMAR|\n",
      "|  2|Priya Reddy|PRIYA REDDY|\n",
      "|  3|   John Doe|   JOHN DOE|\n",
      "|  4|Kiran Kumar|KIRAN KUMAR|\n",
      "|  5|Meera Priya|MEERA PRIYA|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('upper_name', upper('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1da86bc-8900-4036-af8a-e5f9387d83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-------------+\n",
      "|       name|ename_upper|ename_lower|ename_initcap|\n",
      "+-----------+-----------+-----------+-------------+\n",
      "| Ravi Kumar| RAVI KUMAR| ravi kumar|   Ravi Kumar|\n",
      "|Priya Reddy|PRIYA REDDY|priya reddy|  Priya Reddy|\n",
      "|   John Doe|   JOHN DOE|   john doe|     John Doe|\n",
      "|Kiran Kumar|KIRAN KUMAR|kiran kumar|  Kiran Kumar|\n",
      "|Meera Priya|MEERA PRIYA|meera priya|  Meera Priya|\n",
      "+-----------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"name\",\n",
    "    upper(\"name\").alias(\"ename_upper\"),\n",
    "    lower(\"name\").alias(\"ename_lower\"),\n",
    "    initcap(\"name\").alias(\"ename_initcap\")).show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79396bd8-956e-4e2d-bcf3-9250900b1825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+\n",
      "| ename|Name_ltrim|Name_rtrim|Name_trim|\n",
      "+------+----------+----------+---------+\n",
      "| Smith|     Smith|     Smith|    Smith|\n",
      "| Allen|     Allen|     Allen|    Allen|\n",
      "|  Ward|      Ward|      Ward|     Ward|\n",
      "| Jones|     Jones|     Jones|    Jones|\n",
      "|Martin|    Martin|    Martin|   Martin|\n",
      "+------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(\"ename\",\n",
    "          ltrim(\"ename\").alias(\"Name_ltrim\"),\n",
    "          rtrim(\"ename\").alias(\"Name_rtrim\"),\n",
    "          trim(\"ename\").alias(\"Name_trim\")\n",
    "         ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de447f9-8fe4-4c79-a8b3-f67f4f8afb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "| ename|Name_first3|\n",
      "+------+-----------+\n",
      "| Smith|        Smi|\n",
      "| Allen|        All|\n",
      "|  Ward|        War|\n",
      "| Jones|        Jon|\n",
      "|Martin|        Mar|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "emp.select(\n",
    "    \"ename\",\n",
    "    substring(\"ename\", 1, 3).alias(\"Name_first3\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55c36a76-5afd-4fe1-84f0-6a7d524b3cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "| ename|Name_first3|\n",
      "+------+-----------+\n",
      "| Smith|        ith|\n",
      "| Allen|        len|\n",
      "|  Ward|        ard|\n",
      "| Jones|        nes|\n",
      "|Martin|        tin|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(\n",
    "    \"ename\",\n",
    "    substring(\"ename\", -3,3).alias(\"Name_last3\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5de8b363-fe2d-4939-9911-293983967ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "| ename|len|\n",
      "+------+---+\n",
      "| Smith|  5|\n",
      "| Allen|  5|\n",
      "|  Ward|  4|\n",
      "| Jones|  5|\n",
      "|Martin|  6|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(\"ename\", \n",
    "          length(\"ename\").alias(\"len\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7dfc75f-b2ea-48ec-a3fa-e96c1ed50096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|emp_id| ename|dept_id|\n",
      "+------+------+-------+\n",
      "|     1| Smith|     10|\n",
      "|     2| Allen|     20|\n",
      "|     4| Jones|     30|\n",
      "|     5|Martin|   NULL|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.filter(length(\"ename\") >4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd4a6b90-b7ed-4ffc-b1d1-69d75bb71290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  4|Kiran Kumar|\n",
      "|  2|Priya Reddy|\n",
      "|  5|Meera Priya|\n",
      "|  1| Ravi Kumar|\n",
      "|  3|   John Doe|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(length(col(\"name\")).desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fe5c7e6-dbbd-4c9a-8fec-fa9288df5d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|       name|name_substring|\n",
      "+-----------+--------------+\n",
      "| Ravi Kumar|          Ravi|\n",
      "|Priya Reddy|          Priy|\n",
      "|   John Doe|          John|\n",
      "|Kiran Kumar|          Kira|\n",
      "|Meera Priya|          Meer|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "        \"name\",\n",
    "        substring(\"name\", 1, 4).alias(\"name_substring\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "795de7f0-2b3d-4ec6-8052-af422b67625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|       name|name_substring|\n",
      "+-----------+--------------+\n",
      "| Ravi Kumar|       i Kumar|\n",
      "|Priya Reddy|      ya Reddy|\n",
      "|   John Doe|         n Doe|\n",
      "|Kiran Kumar|      an Kumar|\n",
      "|Meera Priya|      ra Priya|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "        \"name\",\n",
    "        substring(\"name\", 4,length(\"name\")).alias(\"name_substring\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0e5a50-fcb1-4856-850d-a88bd976cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|          new|\n",
      "+-------------+\n",
      "| Ravi Kumar_1|\n",
      "|Priya Reddy_2|\n",
      "|   John Doe_3|\n",
      "|Kiran Kumar_4|\n",
      "|Meera Priya_5|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    concat(\"name\", lit(\"_\"), \"id\").alias(\"new\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5982676f-e2aa-4071-b625-5ba16c4cb161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|id_name               |\n",
      "+----------------------+\n",
      "|1|Ravi Kumar|India|TN |\n",
      "|2|Priya Reddy|India|TN|\n",
      "|3|John Doe|India|TN   |\n",
      "|4|Kiran Kumar|India|TN|\n",
      "|5|Meera Priya|India|TN|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    concat_ws(\"|\",\"id\", \"name\",lit('India'),lit(\"TN\")).alias(\"id_name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e247f56b-114c-4a39-9518-f6bc06a8e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          name|\n",
      "+--------------+\n",
      "|Ravindra Kumar|\n",
      "|   Priya Reddy|\n",
      "|      John Doe|\n",
      "|   Kiran Kumar|\n",
      "|   Meera Priya|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\n",
    "    regexp_replace(col(\"name\"), \"Ravi\", \"Ravindra\").alias(\"name\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd91d214-72f1-472f-a057-500cc5fb5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      splitted|\n",
      "+--------------+\n",
      "| [Ravi, Kumar]|\n",
      "|[Priya, Reddy]|\n",
      "|   [John, Doe]|\n",
      "|[Kiran, Kumar]|\n",
      "|[Meera, Priya]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(\"name\", \" \").alias(\"splitted\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da4d55bc-8c11-4058-89e3-cba7d0c64782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|Index|\n",
      "+-----------+-----+\n",
      "| Ravi Kumar|    0|\n",
      "|Priya Reddy|    7|\n",
      "|   John Doe|    0|\n",
      "|Kiran Kumar|    0|\n",
      "|Meera Priya|    0|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp.select(\n",
    "    \"ename\",\n",
    "    instr(\"ename\", \"Reddy\").alias(\"Index\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64dfd656-3dae-4b45-8661-b3b3af999f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+\n",
      "|       name|first_name|last_name|\n",
      "+-----------+----------+---------+\n",
      "| Ravi Kumar|     Ravi |    Kumar|\n",
      "|Priya Reddy|     Priya|  a Reddy|\n",
      "|   John Doe|     John |      Doe|\n",
      "|Kiran Kumar|     Kiran|  n Kumar|\n",
      "|Meera Priya|     Meera|  a Priya|\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", substring(\"name\",1,5).alias(\"first_name\"),\n",
    "                  substring(\"name\",5 ,100).alias(\"last_name\")).show()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4ef5120-9196-4774-bc84-08f751e5d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+----------+---------+\n",
      "|       name|pos|First Name|Last Name|\n",
      "+-----------+---+----------+---------+\n",
      "| Ravi Kumar|  5|      Ravi|    Kumar|\n",
      "|Priya Reddy|  6|     Priya|    Reddy|\n",
      "|   John Doe|  5|      John|      Doe|\n",
      "|Kiran Kumar|  6|     Kiran|    Kumar|\n",
      "|Meera Priya|  6|     Meera|    Priya|\n",
      "+-----------+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"name\",\n",
    "    instr(\"name\",\" \").alias(\"pos\"),\n",
    "    substring(\"name\", 1, instr(\"name\", \" \")-1).alias(\"First Name\"),\n",
    "    substring(\"name\",  instr(\"name\", \" \")+1, length(\"name\")).alias(\"Last Name\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfd814c6-773f-419a-92b8-937514a56c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- delivery_ts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"2025-05-19\", \"2025-12-15 14:25:00\"),\n",
    "    (2, \"2024-05-20\", \"2025-11-01 09:10:30\"),\n",
    "    (3, \"2024-05-21\", \"2025-07-30 20:45:10\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"order_date\", \"delivery_ts\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d202a890-0522-4fc5-903d-13c94f340d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.withColumns( {\"order_date\": to_date(\"order_date\"),\n",
    "                 \"delivery_ts\":to_timestamp(\"delivery_ts\")\n",
    "                }\n",
    "              ) \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c862287-7b80-42c2-ab1b-76d123e7bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- delivery_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9780cf1c-f0f4-4336-9782-2c3ec5e16fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bedd60cc-61da-4078-8147-09ba80d9b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---------------+\n",
      "|current_date|current_date+10|current_date-10|\n",
      "+------------+---------------+---------------+\n",
      "|2026-02-20  |2026-03-02     |2026-02-10     |\n",
      "+------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select ( current_date().alias(\"current_date\"), \n",
    "            date_add(current_date(), 10).alias(\"current_date+10\"),\n",
    "             date_sub(current_date(), 10).alias(\"current_date-10\") \n",
    "           ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "62e130f8-412e-417e-89c8-140821453a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+\n",
      "| id|order_date|        delivery_ts|\n",
      "+---+----------+-------------------+\n",
      "|  1|2025-05-19|2025-12-15 14:25:00|\n",
      "|  2|2024-05-20|2025-11-01 09:10:30|\n",
      "|  3|2024-05-21|2025-07-30 20:45:10|\n",
      "+---+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c154b412-484f-4144-91c9-2b281ccd421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|order_date|add_10_days|sub_5_days|\n",
      "+----------+-----------+----------+\n",
      "|2025-05-19| 2025-05-29|2025-05-14|\n",
      "|2024-05-20| 2024-05-30|2024-05-15|\n",
      "|2024-05-21| 2024-05-31|2024-05-16|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "     \"order_date\",\n",
    "    date_add(\"order_date\", 10).alias(\"add_10_days\"),\n",
    "    date_sub(\"order_date\", 5).alias(\"sub_5_days\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77d66063-c497-4248-9201-e24d6872e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|order_date|add_2_months|\n",
      "+----------+------------+\n",
      "|2025-05-19|  2025-07-19|\n",
      "|2024-05-20|  2024-07-20|\n",
      "|2024-05-21|  2024-07-21|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"order_date\",\n",
    "    add_months(\"order_date\", 2).alias(\"add_2_months\")\n",
    "    \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d223741f-2045-4b95-a5b3-91b3fe9cf186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------------------------------+\n",
      "|        delivery_ts|order_date|date_diff(delivery_ts, order_date)|\n",
      "+-------------------+----------+----------------------------------+\n",
      "|2025-12-15 14:25:00|2025-05-19|                               210|\n",
      "|2025-11-01 09:10:30|2024-05-20|                               530|\n",
      "|2025-07-30 20:45:10|2024-05-21|                               435|\n",
      "+-------------------+----------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select( \"delivery_ts\",\n",
    "           \"order_date\",\n",
    "            date_diff(col(\"delivery_ts\") ,col(\"order_date\"))\n",
    "        \n",
    "    \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a366922c-2e2e-45dc-98a6-ac9dff45bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+------+-----+-----------+---+-----------+------------+-----------------+----+------+------+\n",
      "|order_date|dayname|year|quater|month|day of week|day|day of year|week of year|last_day_in_month|hour|minute|second|\n",
      "+----------+-------+----+------+-----+-----------+---+-----------+------------+-----------------+----+------+------+\n",
      "|2025-05-19|    Mon|2025|     2|    5|          2| 19|        139|          21|       2025-05-31|  14|    25|     0|\n",
      "|2024-05-20|    Mon|2024|     2|    5|          2| 20|        141|          21|       2024-05-31|   9|    10|    30|\n",
      "|2024-05-21|    Tue|2024|     2|    5|          3| 21|        142|          21|       2024-05-31|  20|    45|    10|\n",
      "+----------+-------+----+------+-----+-----------+---+-----------+------------+-----------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"order_date\",\n",
    "    dayname(\"order_date\").alias(\"dayname\"),\n",
    "    year(\"order_date\").alias(\"year\"),\n",
    "    quarter(\"order_date\").alias(\"quater\"),\n",
    "    month(\"order_date\").alias(\"month\"),\n",
    "    dayofweek(\"order_date\").alias(\"day of week\"),\n",
    "    dayofmonth(\"order_date\").alias(\"day\"),\n",
    "    dayofyear(\"order_date\").alias(\"day of year\"),\n",
    "    weekofyear(\"order_date\").alias(\"week of year\"),\n",
    "    last_day(\"order_date\").alias(\"last_day_in_month\"),\n",
    "    hour(\"delivery_ts\").alias(\"hour\"),\n",
    "    minute(\"delivery_ts\").alias(\"minute\"),\n",
    "    second(\"delivery_ts\").alias(\"second\"),\n",
    "    \n",
    "    \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbcd589c-ef15-473c-ac92-44b5592ce1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|dd/mm/yyyy|year|\n",
      "+----------+----+\n",
      "|19/05/2025|  05|\n",
      "|20/05/2024|  05|\n",
      "|21/05/2024|  05|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select( date_format(\"order_date\",'dd/MM/yyyy').alias(\"dd/mm/yyyy\"),\n",
    "           date_format(\"order_date\",'MM').alias(\"year\"),\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "723a3cdf-6f34-4ba5-abb0-ad6977f4fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+---------+\n",
      "|        delivery_ts|order_date|days_diff|\n",
      "+-------------------+----------+---------+\n",
      "|2025-12-15 14:25:00|2025-05-19|      210|\n",
      "|2025-11-01 09:10:30|2024-05-20|      530|\n",
      "|2025-07-30 20:45:10|2024-05-21|      435|\n",
      "+-------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"delivery_ts\",\n",
    "    \"order_date\",\n",
    "    datediff(\"delivery_ts\", \"order_date\").alias(\"days_diff\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0a3773e6-7328-46e6-b7e6-d385751b8fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|first_day_month|first_day_year|\n",
      "+---------------+--------------+\n",
      "|     2025-05-01|          NULL|\n",
      "|     2024-05-01|          NULL|\n",
      "|     2024-05-01|          NULL|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    trunc(\"order_date\", \"MM\").alias(\"first_day_month\"),\n",
    "    trunc(\"order_date\", \"YY\").alias(\"first_day_year\"),\n",
    "   \n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3a361ad5-ec6e-4e73-a1dc-db0d49c6775c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+----+----------+-----------+\n",
      "|date_full|date_full|month_full|month_full|qter|qtrer_2dig| month_full|\n",
      "+---------+---------+----------+----------+----+----------+-----------+\n",
      "| Thursday|      Thu|  December|       Dec|   4|        04|4th quarter|\n",
      "+---------+---------+----------+----------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    date_format(current_date(), \"EEEE\").alias(\"date_full\"),\n",
    "    date_format(current_date(), \"EEE\").alias(\"date_full\"),\n",
    "    date_format(current_date(), \"MMMM\").alias(\"month_full\"),\n",
    "    date_format(current_date(), \"MMM\").alias(\"month_full\"),\n",
    "    date_format(current_date(), \"Q\").alias(\"qter\"),\n",
    "    date_format(current_date(), \"QQ\").alias(\"qtrer_2dig\"),\n",
    "    date_format(current_date(), \"QQQQ\").alias(\"month_full\")\n",
    "      \n",
    "        ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c265b92-0fbc-41ab-8581-e50c7dc584fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----+\n",
      "| id|amount|discount|value|\n",
      "+---+------+--------+-----+\n",
      "|  1| 10.75|      -5|  100|\n",
      "|  2|  20.4|     -15|  400|\n",
      "|  3|  30.9|      12|  900|\n",
      "+---+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 10.75, -5, 100),\n",
    "    (2, 20.40, -15, 400),\n",
    "    (3, 30.90, 12, 900)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"amount\", \"discount\", \"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f008075e-fbad-49dd-acc1-45cee1560ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|amount|round0|round1|\n",
      "+------+------+------+\n",
      "| 10.75|  11.0|  10.8|\n",
      "|  20.4|  20.0|  20.4|\n",
      "|  30.9|  31.0|  30.9|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"amount\", \n",
    "          round(\"amount\",0).alias(\"round0\"),\n",
    "         round(\"amount\",1).alias(\"round1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d212f3da-d592-4328-90fe-71bb81d2a468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+\n",
      "|amount|ceil|floor|\n",
      "+------+----+-----+\n",
      "| 10.75|  11|   10|\n",
      "|  20.4|  21|   20|\n",
      "|  30.9|  31|   30|\n",
      "+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"amount\",\n",
    "    ceil(\"amount\").alias(\"ceil\"), \n",
    "    floor(\"amount\").alias(\"floor\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feec5915-156e-4f94-a03b-7c46083887c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|discount|abs_value|\n",
      "+--------+---------+\n",
      "|      -5|        5|\n",
      "|     -15|       15|\n",
      "|      12|       12|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"discount\", \n",
    "    abs(\"discount\").alias(\"abs_value\")\n",
    "   ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3c7f73bb-2aa7-4018-91d7-0a1000756c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----+\n",
      "|value|sqrt_via_pow|sqrt|\n",
      "+-----+------------+----+\n",
      "|  100|     10000.0|10.0|\n",
      "|  400|    160000.0|20.0|\n",
      "|  900|    810000.0|30.0|\n",
      "+-----+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"value\",\n",
    "    pow(\"value\", 2).alias(\"sqrt_via_pow\"), \n",
    "    sqrt(\"value\").alias(\"sqrt\")\n",
    "         ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f5438b60-be2d-479c-aa0f-17f7a8f66226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------------+------------------+--------------------+\n",
      "|value|               ln|              ln2|              ln10|           exp_value|\n",
      "+-----+-----------------+-----------------+------------------+--------------------+\n",
      "|  100|4.605170185988092|6.643856189774725|               2.0|  46630.028453524326|\n",
      "|  400|5.991464547107982|8.643856189774725|2.6020599913279625| 7.237814209482772E8|\n",
      "|  900|6.802394763324311|9.813781191217037|2.9542425094393248|2.628448612801719E13|\n",
      "+-----+-----------------+-----------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"value\",\n",
    "    log(\"value\").alias(\"ln\"),\n",
    "    log2(\"value\").alias(\"ln2\"),\n",
    "    log10(\"value\").alias(\"ln10\"),\n",
    "    exp(\"amount\").alias(\"exp_value\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5a8c8ed5-182a-4c9a-80e8-4dff6c28327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|greatest|least|\n",
      "+--------+-----+\n",
      "|   100.0| -5.0|\n",
      "|   400.0|-15.0|\n",
      "|   900.0| 12.0|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "        greatest(\"amount\", \"discount\", \"value\").alias(\"greatest\"),\n",
    "       least(\"amount\", \"discount\", \"value\").alias(\"least\")\n",
    "       ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee1e6aff-6712-4101-92a3-23dff22db536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+-------------------+-----+------------------+\n",
      "|     key|count| max|               mean|  min|            stddev|\n",
      "+--------+-----+----+-------------------+-----+------------------+\n",
      "|  amount|    3|30.9| 20.683333333333334|10.75|10.077987563662367|\n",
      "|discount|    3|  12|-2.6666666666666665|  -15|13.650396819628847|\n",
      "|   value|    3| 900|  466.6666666666667|  100|404.14518843273805|\n",
      "+--------+-----+----+-------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"amount\", \"discount\", \"value\").transpose().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a6561dc-cd67-43e3-a676-ff83abb2e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------+-----------+------------------+\n",
      "|sum(amount)|       avg(amount)|min(amount)|max(amount)|    stddev(amount)|\n",
      "+-----------+------------------+-----------+-----------+------------------+\n",
      "|      62.05|20.683333333333334|      10.75|       30.9|10.077987563662367|\n",
      "+-----------+------------------+-----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    sum(\"amount\"),\n",
    "    avg(\"amount\"),\n",
    "    min(\"amount\"),\n",
    "    max(\"amount\"),\n",
    "    stddev(\"amount\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b2a4544-85d3-4c95-9766-0cc88b07fec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+--------+-----+\n",
      "|    row_id| id|amount|discount|value|\n",
      "+----------+---+------+--------+-----+\n",
      "|         0|  1| 10.75|      -5|  100|\n",
      "|8589934592|  2|  20.4|     -15|  400|\n",
      "|8589934593|  3|  30.9|      12|  900|\n",
      "+----------+---+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    monotonically_increasing_id().alias(\"row_id\"), \n",
    "    \"*\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "761348eb-51be-4318-a7e6-7cf9532b3a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+\n",
      "| id| name|   dept|salary|\n",
      "+---+-----+-------+------+\n",
      "|101| Ravi|     HR| 50000|\n",
      "|102|Priya|Finance| 60000|\n",
      "|103| John|     IT| 45000|\n",
      "|104|Kiran|     HR| 55000|\n",
      "|105|Meera|Finance| 65000|\n",
      "|106| Ajay|     IT| 48000|\n",
      "+---+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (101, \"Ravi\",   \"HR\",       50000),\n",
    "    (102, \"Priya\",  \"Finance\",  60000),\n",
    "    (103, \"John\",   \"IT\",       45000),\n",
    "    (104, \"Kiran\",  \"HR\",       55000),\n",
    "    (105, \"Meera\",  \"Finance\",  65000),\n",
    "    (106, \"Ajay\",   \"IT\",       48000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7747033e-c20b-4876-9f64-1ce11baa8d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   dept|Total_Sal|\n",
      "+-------+---------+\n",
      "|     HR|   105000|\n",
      "|Finance|   125000|\n",
      "|     IT|    93000|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"Total_Sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "956ce032-4121-4b0b-a980-3c2b0b04d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+-------+-------+---------+\n",
      "|   dept|total_sal|max_sal|min_sal|avg_sal|count_sal|\n",
      "+-------+---------+-------+-------+-------+---------+\n",
      "|     HR|   105000|  55000|  50000|52500.0|        2|\n",
      "|Finance|   125000|  65000|  60000|62500.0|        2|\n",
      "+-------+---------+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_sal\"),\n",
    "                       max(\"salary\").alias(\"max_sal\"),\n",
    "                       min(\"salary\").alias(\"min_sal\"),\n",
    "                       avg(\"salary\").alias(\"avg_sal\"),\n",
    "                       count(\"salary\").alias(\"count_sal\")).filter(col(\"total_sal\") >100000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f345010d-842d-4156-ab6e-83cf860cd3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   dept|avg_sal|\n",
      "+-------+-------+\n",
      "|Finance|62500.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"dept\") \\\n",
    "  .agg(avg(\"salary\").alias(\"avg_sal\")) \\\n",
    "  .filter(col(\"avg_sal\") > 55000) \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7411bf8-5c48-4ec8-b338-1e30720a0f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-----+\n",
      "| name|Finance|   HR|   IT|\n",
      "+-----+-------+-----+-----+\n",
      "| Ravi|      0|50000|    0|\n",
      "|Meera|  65000|    0|    0|\n",
      "| Ajay|      0|    0|48000|\n",
      "|Priya|  60000|    0|    0|\n",
      "| John|      0|    0|45000|\n",
      "|Kiran|      0|55000|    0|\n",
      "+-----+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"name\").pivot(\"dept\").agg(sum(\"salary\")).fillna(0).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fae5e4e8-df7e-4fa4-979e-05da3d1d6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "col='order_id int, order_date timestamp, customer_id int, status string'\n",
    "ord_df = spark.read.csv('c:\\data\\Orders',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bb92628d-f200-4b65-8502-0c74fae7465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+\n",
      "|         status|order_year|order_count|\n",
      "+---------------+----------+-----------+\n",
      "|        ON_HOLD|      2014|       2147|\n",
      "|PENDING_PAYMENT|      2013|       6749|\n",
      "|        ON_HOLD|      2013|       1651|\n",
      "|        PENDING|      2013|       3380|\n",
      "|       CANCELED|      2014|        791|\n",
      "|        PENDING|      2014|       4230|\n",
      "|       COMPLETE|      2014|      12749|\n",
      "|     PROCESSING|      2013|       3617|\n",
      "|PENDING_PAYMENT|      2014|       8281|\n",
      "|SUSPECTED_FRAUD|      2014|        862|\n",
      "|SUSPECTED_FRAUD|      2013|        696|\n",
      "|     PROCESSING|      2014|       4658|\n",
      "| PAYMENT_REVIEW|      2014|        421|\n",
      "|         CLOSED|      2014|       4082|\n",
      "| PAYMENT_REVIEW|      2013|        308|\n",
      "|         CLOSED|      2013|       3474|\n",
      "|       COMPLETE|      2013|      10150|\n",
      "|       CANCELED|      2013|        637|\n",
      "+---------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.groupBy(\"status\", year(\"order_date\").alias(\"order_year\"))\\\n",
    "    .agg(count(\"status\").alias(\"order_count\")).show()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2cd374b3-77aa-451e-9814-67dc4edba654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+-------+-------+---------+\n",
      "|   dept|total_sal|max_sal|min_sal|avg_sal|count_sal|\n",
      "+-------+---------+-------+-------+-------+---------+\n",
      "|     HR|   105000|  55000|  50000|52500.0|        2|\n",
      "|Finance|   125000|  65000|  60000|62500.0|        2|\n",
      "|     IT|    93000|  48000|  45000|46500.0|        2|\n",
      "+-------+---------+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg=df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_sal\"),\n",
    "                       max(\"salary\").alias(\"max_sal\"),\n",
    "                       min(\"salary\").alias(\"min_sal\"),\n",
    "                       avg(\"salary\").alias(\"avg_sal\"),\n",
    "                       count(\"salary\").alias(\"count_sal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c68bab43-2015-4fbe-98c5-9a75a422d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_df =spark.read.csv('c:/data/Orders',\n",
    "               \"order_id int,order_date date,customer_id int, order_status string\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b8b6732c-ef32-4f38-973d-7517a381f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-25|      11599|         CLOSED|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|      12111|       COMPLETE|\n",
      "|       4|2013-07-25|       8827|         CLOSED|\n",
      "|       5|2013-07-25|      11318|       COMPLETE|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "ord_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "86df6bf5-d560-43a9-9ba8-a06e81ea43a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|   order_status|count|\n",
      "+---------------+-----+\n",
      "|PENDING_PAYMENT|15030|\n",
      "|       COMPLETE|22899|\n",
      "|        ON_HOLD| 3798|\n",
      "| PAYMENT_REVIEW|  729|\n",
      "|     PROCESSING| 8275|\n",
      "|         CLOSED| 7556|\n",
      "|SUSPECTED_FRAUD| 1558|\n",
      "|        PENDING| 7610|\n",
      "|       CANCELED| 1428|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord_df.groupBy(\"order_status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4c499797-fc6f-469a-bc94-770b4feff4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "col=\"order_item_id int ,order_id int ,product_id int,quantity int,sub_total float,price float\"\n",
    "ordItems_df =spark.read.csv(\"c:/data/OrderItems\",col\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "69ab7709-1bb1-4c6f-8470-b2bbc36588f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+---------+------+\n",
      "|order_item_id|order_id|product_id|quantity|sub_total| price|\n",
      "+-------------+--------+----------+--------+---------+------+\n",
      "|            1|       1|       957|       1|   299.98|299.98|\n",
      "|            2|       2|      1073|       1|   199.99|199.99|\n",
      "|            3|       2|       502|       5|    250.0|  50.0|\n",
      "|            4|       2|       403|       1|   129.99|129.99|\n",
      "|            5|       4|       897|       2|    49.98| 24.99|\n",
      "+-------------+--------+----------+--------+---------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "ordItems_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a444f696-6127-4c47-8640-a4492fa19b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = ord_df.join(ordItems_df , \"order_id\",\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "de9fbb6f-788f-4553-90e5-a06d3fd19acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+-------------+----------+--------+---------+------+\n",
      "|order_id|order_date|customer_id|   order_status|order_item_id|product_id|quantity|sub_total| price|\n",
      "+--------+----------+-----------+---------------+-------------+----------+--------+---------+------+\n",
      "|       1|2013-07-25|      11599|         CLOSED|            1|       957|       1|   299.98|299.98|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|            2|      1073|       1|   199.99|199.99|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|            3|       502|       5|    250.0|  50.0|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|            4|       403|       1|   129.99|129.99|\n",
      "+--------+----------+-----------+---------------+-------------+----------+--------+---------+------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "join_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3cd16e09-23cd-4a42-8608-e7b532a126ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+---------+\n",
      "|status         |total_revenue|total_qty|\n",
      "+---------------+-------------+---------+\n",
      "|PENDING_PAYMENT|7581671.20   |82935    |\n",
      "|COMPLETE       |11276933.91  |123873   |\n",
      "|ON_HOLD        |1864731.28   |20414    |\n",
      "|PAYMENT_REVIEW |357841.46    |4013     |\n",
      "|PROCESSING     |4190636.84   |45622    |\n",
      "|CLOSED         |3736048.86   |40510    |\n",
      "|SUSPECTED_FRAUD|766844.69    |8429     |\n",
      "|PENDING        |3851881.36   |42260    |\n",
      "|CANCELED       |696031.00    |7702     |\n",
      "+---------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agg_df = join_df.groupBy(\"status\") \\\n",
    "    .agg(\n",
    "        sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "    )\n",
    "\n",
    "agg_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ebdb9b1-c3e6-42cc-accb-9cb4dceda590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+---------+\n",
      "|order_status|product_id|total_revenue|total_qty|\n",
      "+------------+----------+-------------+---------+\n",
      "|CANCELED    |823       |831.84       |16       |\n",
      "|CANCELED    |715       |259.98       |2        |\n",
      "|CANCELED    |775       |219.78       |22       |\n",
      "|CANCELED    |926       |271.83       |17       |\n",
      "|CANCELED    |191       |80691.93     |807      |\n",
      "|CANCELED    |135       |418.00       |19       |\n",
      "|CANCELED    |771       |479.88       |12       |\n",
      "|CANCELED    |249       |989.46       |18       |\n",
      "|CANCELED    |957       |80094.66     |267      |\n",
      "|CANCELED    |825       |511.84       |16       |\n",
      "|CANCELED    |365       |85785.70     |1430     |\n",
      "|CANCELED    |359       |699.93       |7        |\n",
      "|CANCELED    |642       |870.00       |29       |\n",
      "|CANCELED    |306       |629.93       |7        |\n",
      "|CANCELED    |134       |325.00       |13       |\n",
      "|CANCELED    |885       |524.79       |21       |\n",
      "|CANCELED    |728       |2145.00      |33       |\n",
      "|CANCELED    |828       |447.86       |14       |\n",
      "|CANCELED    |235       |664.81       |19       |\n",
      "|CANCELED    |893       |74.97        |3        |\n",
      "+------------+----------+-------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "agg_df = join_df.groupBy(\"order_status\",\"product_id\") \\\n",
    "    .agg(\n",
    "        sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "    ).orderBy(\"order_status\")\n",
    "\n",
    "agg_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "61224e95-0a4e-4e8c-b754-40c40426ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------+\n",
      "|order_date|total_revenue|total_qty|\n",
      "+----------+-------------+---------+\n",
      "|2013-09-09|124454.49    |1336     |\n",
      "|2013-09-19|99324.38     |1100     |\n",
      "|2014-06-03|58634.88     |632      |\n",
      "|2013-09-12|89120.48     |951      |\n",
      "|2014-01-24|86299.03     |944      |\n",
      "|2014-02-16|116039.55    |1262     |\n",
      "|2014-06-11|71742.39     |795      |\n",
      "|2013-11-18|94744.59     |1038     |\n",
      "|2014-02-18|104887.89    |1252     |\n",
      "|2013-08-14|103939.26    |1093     |\n",
      "|2013-10-05|105913.05    |1156     |\n",
      "|2014-07-04|80105.76     |895      |\n",
      "|2014-07-06|54898.37     |618      |\n",
      "|2013-09-18|115217.08    |1313     |\n",
      "|2013-09-20|82662.51     |900      |\n",
      "|2013-09-25|141775.64    |1609     |\n",
      "|2014-06-13|128392.34    |1391     |\n",
      "|2013-11-23|128024.84    |1446     |\n",
      "|2013-09-14|135308.52    |1518     |\n",
      "|2014-02-24|93628.83     |1028     |\n",
      "+----------+-------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "agg_df = join_df.groupBy(\"order_date\") \\\n",
    "    .agg(\n",
    "        sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "    )\n",
    "\n",
    "agg_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9f4cab8c-89e8-4263-871c-44a2420c9289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+---------+\n",
      "|order_year|total_revenue|total_qty|\n",
      "+----------+-------------+---------+\n",
      "|2013      |15254189.37  |167409   |\n",
      "|2014      |19068431.23  |208349   |\n",
      "+----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = join_df.groupBy(year(\"order_date\").alias(\"order_year\")) \\\n",
    "    .agg(\n",
    "        sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "    )\n",
    "\n",
    "agg_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4fd2c9b4-56dd-40b5-a6f1-26fbedf3313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+---------+\n",
      "|order_year|order_month|total_revenue|total_qty|\n",
      "+----------+-----------+-------------+---------+\n",
      "|      2013|        Jul|    764782.20|     8421|\n",
      "|      2013|        Aug|   2828658.75|    31311|\n",
      "|      2013|        Sep|   2934527.33|    32147|\n",
      "|      2013|        Oct|   2624600.66|    28904|\n",
      "|      2013|        Nov|   3168656.09|    34705|\n",
      "|      2013|        Dec|   2932964.33|    31921|\n",
      "|      2014|        Jan|   2924447.07|    31769|\n",
      "|      2014|        Feb|   2778663.71|    30638|\n",
      "|      2014|        Mar|   2862492.27|    31221|\n",
      "|      2014|        Apr|   2807789.85|    30820|\n",
      "|      2014|        May|   2753078.27|    30104|\n",
      "|      2014|        Jun|   2703463.49|    29466|\n",
      "|      2014|        Jul|   2238496.56|    24331|\n",
      "+----------+-----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = join_df.groupBy(year(\"order_date\").alias(\"order_year\"),\n",
    "                         monthname(\"order_date\").alias(\"order_month\"),\n",
    "                        month(\"order_date\")) \\\n",
    "    .agg(\n",
    "        sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "    ).orderBy(year(\"order_date\"), month(\"order_date\"))\n",
    "\n",
    "agg_df.drop(\"month(order_date)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a2f47096-a0c3-4b35-ae6b-a971822c4721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+---------+\n",
      "|order_year|order_month|total_revenue|total_qty|\n",
      "+----------+-----------+-------------+---------+\n",
      "|      2013|        Jul|    764782.20|     8421|\n",
      "|      2013|        Aug|   2828658.75|    31311|\n",
      "|      2013|        Sep|   2934527.33|    32147|\n",
      "|      2013|        Oct|   2624600.66|    28904|\n",
      "|      2013|        Nov|   3168656.09|    34705|\n",
      "|      2013|        Dec|   2932964.33|    31921|\n",
      "|      2014|        Jan|   2924447.07|    31769|\n",
      "|      2014|        Feb|   2778663.71|    30638|\n",
      "|      2014|        Mar|   2862492.27|    31221|\n",
      "|      2014|        Apr|   2807789.85|    30820|\n",
      "|      2014|        May|   2753078.27|    30104|\n",
      "|      2014|        Jun|   2703463.49|    29466|\n",
      "|      2014|        Jul|   2238496.56|    24331|\n",
      "+----------+-----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = join_df.groupBy(year(\"order_date\").alias(\"order_year\"),\n",
    "                         monthname(\"order_date\").alias(\"order_month\"),\n",
    "                        month(\"order_date\").alias(\"order_month_no\")) \\\n",
    "    .agg(\n",
    "        sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_qty\"),\n",
    "    ).orderBy(\"order_year\",\"order_month_no\")\n",
    "agg_df.drop(\"order_month_no\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "94da4d4c-51e1-4da8-9b4d-1ee3f291169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+---------+\n",
      "|order_year|order_month|total_revenue|total_qty|\n",
      "+----------+-----------+-------------+---------+\n",
      "|2013      |Nov        |3168656.09   |34705    |\n",
      "|2014      |May        |2753078.27   |30104    |\n",
      "|2013      |Aug        |2828658.75   |31311    |\n",
      "|2014      |Mar        |2862492.27   |31221    |\n",
      "|2013      |Sep        |2934527.33   |32147    |\n",
      "|2014      |Feb        |2778663.71   |30638    |\n",
      "|2014      |Apr        |2807789.85   |30820    |\n",
      "|2013      |Dec        |2932964.33   |31921    |\n",
      "|2014      |Jan        |2924447.07   |31769    |\n",
      "+----------+-----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, monthname, sum, col\n",
    "\n",
    "result_df = (\n",
    "    join_df\n",
    "        .groupBy(\n",
    "            year(\"order_date\").alias(\"order_year\"),\n",
    "            monthname(\"order_date\").alias(\"order_month\")\n",
    "        )\n",
    "        .agg(\n",
    "            sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "            sum(\"quantity\").alias(\"total_qty\")\n",
    "        )\n",
    "        .filter(col(\"total_qty\") > 30000)     \n",
    ")\n",
    "\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b69da5a1-a94c-4fbb-acdc-0fb59b0700ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-------------+---------+\n",
      "|order_year|order_qtr|order_month|total_revenue|total_qty|\n",
      "+----------+---------+-----------+-------------+---------+\n",
      "|2013      |3        |August     |2828658.75   |31311    |\n",
      "|2013      |3        |July       |764782.20    |8421     |\n",
      "|2013      |3        |September  |2934527.33   |32147    |\n",
      "|2013      |4        |December   |2932964.33   |31921    |\n",
      "|2013      |4        |November   |3168656.09   |34705    |\n",
      "|2013      |4        |October    |2624600.66   |28904    |\n",
      "|2014      |1        |February   |2778663.71   |30638    |\n",
      "|2014      |1        |January    |2924447.07   |31769    |\n",
      "|2014      |1        |March      |2862492.27   |31221    |\n",
      "|2014      |2        |April      |2807789.85   |30820    |\n",
      "|2014      |2        |June       |2703463.49   |29466    |\n",
      "|2014      |2        |May        |2753078.27   |30104    |\n",
      "|2014      |3        |July       |2238496.56   |24331    |\n",
      "+----------+---------+-----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_df = (\n",
    "    join_df\n",
    "        .groupBy(\n",
    "            year(\"order_date\").alias(\"order_year\"),\n",
    "            \n",
    "            quarter(\"order_date\").alias(\"order_qtr\"),\n",
    "            \n",
    "            \n",
    "            date_format(\"order_date\", \"MMMM\").alias(\"order_month\")   # FULL MONTH NAME\n",
    "        )\n",
    "        .agg(\n",
    "            sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "            sum(\"quantity\").alias(\"total_qty\")\n",
    "        ).orderBy(\"order_year\", \"order_qtr\",\"order_month\")   # HAVING\n",
    ")\n",
    "\n",
    "result_df.show(40,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "76506f70-ad83-4d7e-85e4-45da67070ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-------------+---------+\n",
      "|order_year|order_qtr|order_month|total_revenue|total_qty|\n",
      "+----------+---------+-----------+-------------+---------+\n",
      "|2013      |3        |July       |764782.20    |8421     |\n",
      "|2013      |3        |August     |2828658.75   |31311    |\n",
      "|2013      |3        |September  |2934527.33   |32147    |\n",
      "|2013      |4        |October    |2624600.66   |28904    |\n",
      "|2013      |4        |November   |3168656.09   |34705    |\n",
      "|2013      |4        |December   |2932964.33   |31921    |\n",
      "|2014      |1        |January    |2924447.07   |31769    |\n",
      "|2014      |1        |February   |2778663.71   |30638    |\n",
      "|2014      |1        |March      |2862492.27   |31221    |\n",
      "|2014      |2        |April      |2807789.85   |30820    |\n",
      "|2014      |2        |May        |2753078.27   |30104    |\n",
      "|2014      |2        |June       |2703463.49   |29466    |\n",
      "|2014      |3        |July       |2238496.56   |24331    |\n",
      "+----------+---------+-----------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_df = (\n",
    "    join_df\n",
    "        .groupBy(\n",
    "            year(\"order_date\").alias(\"order_year\"),\n",
    "            quarter(\"order_date\").alias(\"order_qtr\"),\n",
    "            \n",
    "            date_format(\"order_date\", \"MMMM\").alias(\"order_month\"),   # Full month\n",
    "            month(\"order_date\").alias(\"order_month_num\")              # Numeric month for sorting\n",
    "        )\n",
    "        .agg(\n",
    "            sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "            sum(\"quantity\").alias(\"total_qty\")\n",
    "        )\n",
    "        .orderBy(\"order_year\",\"order_qtr\", \"order_month_num\")  # Correct month ordering\n",
    "        .drop(\"order_month_num\")                   # Clean final output\n",
    ")\n",
    "\n",
    "result_df.show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "49017b8b-d79e-4628-b42f-781882dd18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+---------+\n",
      "|order_year|order_qtr|total_revenue|total_qty|\n",
      "+----------+---------+-------------+---------+\n",
      "|2013      |3        |6527968.29   |71879    |\n",
      "|2013      |4        |8726221.08   |95530    |\n",
      "|2014      |1        |8565603.05   |93628    |\n",
      "|2014      |2        |8264331.62   |90390    |\n",
      "|2014      |3        |2238496.56   |24331    |\n",
      "+----------+---------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = (\n",
    "    join_df\n",
    "        .groupBy(\n",
    "            year(\"order_date\").alias(\"order_year\"),\n",
    "            quarter(\"order_date\").alias(\"order_qtr\"),\n",
    "            \n",
    "            \n",
    "        )\n",
    "        .agg(\n",
    "            sum(\"sub_total\").cast(\"decimal(20,2)\").alias(\"total_revenue\"),\n",
    "            sum(\"quantity\").alias(\"total_qty\")\n",
    "        )\n",
    "        .orderBy(\"order_year\",\"order_qtr\" )  # Correct month ordering\n",
    "        \n",
    ")\n",
    "\n",
    "result_df.show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f20d9d-53e0-41ce-9d5b-1fa576d5e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+\n",
      "| id| name|   dept|salary|\n",
      "+---+-----+-------+------+\n",
      "|  1| Ravi|     HR| 50000|\n",
      "|  2| Ravi|     HR| 52000|\n",
      "|  3|Priya|Finance| 60000|\n",
      "|  4| John|     IT| 45000|\n",
      "|  5| John|     IT| 48000|\n",
      "|  6| John|     IT| 50000|\n",
      "+---+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, \"Ravi\", \"HR\", 50000),\n",
    "    (2, \"Ravi\", \"HR\", 52000),\n",
    "    (3, \"Priya\", \"Finance\", 60000),\n",
    "    (4, \"John\", \"IT\", 45000),\n",
    "    (5, \"John\", \"IT\", 48000),\n",
    "    (6, \"John\", \"IT\", 50000),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caf081a-18aa-4068-b0f6-492bd0106a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fde4a24-1e2d-4947-b9d9-aa1db28448cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+----------+\n",
      "| id| name|   dept|salary|row_number|\n",
      "+---+-----+-------+------+----------+\n",
      "|  3|Priya|Finance| 60000|         1|\n",
      "|  2| Ravi|     HR| 52000|         2|\n",
      "|  1| Ravi|     HR| 50000|         3|\n",
      "|  6| John|     IT| 50000|         4|\n",
      "|  5| John|     IT| 48000|         5|\n",
      "|  4| John|     IT| 45000|         6|\n",
      "+---+-----+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w =Window.orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"row_number\", row_number().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1987205b-9bc6-4168-acd9-50c1dd091bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+----------+\n",
      "| id| name|   dept|salary|row_number|\n",
      "+---+-----+-------+------+----------+\n",
      "|  3|Priya|Finance| 60000|         1|\n",
      "|  1| Ravi|     HR| 50000|         1|\n",
      "|  2| Ravi|     HR| 52000|         2|\n",
      "|  4| John|     IT| 45000|         1|\n",
      "|  5| John|     IT| 48000|         2|\n",
      "|  6| John|     IT| 50000|         3|\n",
      "+---+-----+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w =Window.partitionBy(\"dept\"). orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\", row_number().over(w)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a59b6b7c-8c8c-40ef-8dd6-e9f4fe27d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+--------+\n",
      "| id| name|   dept|salary|emp_rank|\n",
      "+---+-----+-------+------+--------+\n",
      "|  3|Priya|Finance| 60000|       1|\n",
      "|  2| Ravi|     HR| 52000|       1|\n",
      "|  1| Ravi|     HR| 50000|       2|\n",
      "|  6| John|     IT| 50000|       1|\n",
      "|  5| John|     IT| 48000|       2|\n",
      "|  4| John|     IT| 45000|       3|\n",
      "+---+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.withColumn(\"emp_rank\", rank().over(w)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5236498-af52-47cb-ad9e-c923294d75d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+-------+\n",
      "| id| name|   dept|salary|row_num|\n",
      "+---+-----+-------+------+-------+\n",
      "|  3|Priya|Finance| 60000|      1|\n",
      "|  2| Ravi|     HR| 52000|      2|\n",
      "|  1| Ravi|     HR| 50000|      3|\n",
      "|  6| John|     IT| 50000|      4|\n",
      "|  5| John|     IT| 48000|      5|\n",
      "|  4| John|     IT| 45000|      6|\n",
      "+---+-----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    row_number().over(w).alias(\"row_num\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b49cf89f-cab2-4b85-9b66-564127a65a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+-------+\n",
      "| id| name|   dept|salary|row_num|\n",
      "+---+-----+-------+------+-------+\n",
      "|  3|Priya|Finance| 60000|      1|\n",
      "|  2| Ravi|     HR| 52000|      1|\n",
      "|  1| Ravi|     HR| 50000|      2|\n",
      "|  6| John|     IT| 50000|      1|\n",
      "|  5| John|     IT| 48000|      2|\n",
      "|  4| John|     IT| 45000|      3|\n",
      "+---+-----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    row_number().over(w).alias(\"row_num\")\n",
    ").show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "542758fe-9b2e-49da-83bf-98522c4f88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+--------+\n",
      "| id| name|   dept|salary|emp_rank|\n",
      "+---+-----+-------+------+--------+\n",
      "|  3|Priya|Finance| 60000|       1|\n",
      "|  2| Ravi|     HR| 52000|       2|\n",
      "|  1| Ravi|     HR| 50000|       3|\n",
      "|  6| John|     IT| 50000|       3|\n",
      "|  5| John|     IT| 48000|       4|\n",
      "|  4| John|     IT| 45000|       5|\n",
      "+---+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w =Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    " dense_rank().over(w).alias(\"emp_rank\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b068166d-f15e-4187-a5b3-cce205c31499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+----+\n",
      "| id| name|   dept|salary|rank|\n",
      "+---+-----+-------+------+----+\n",
      "|  3|Priya|Finance| 60000|   1|\n",
      "|  2| Ravi|     HR| 52000|   1|\n",
      "|  1| Ravi|     HR| 50000|   2|\n",
      "|  6| John|     IT| 50000|   1|\n",
      "|  5| John|     IT| 48000|   2|\n",
      "|  4| John|     IT| 45000|   3|\n",
      "+---+-----+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    rank().over(w).alias(\"rank\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f8760d17-a993-4ff5-b78a-336c89846e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+----------+\n",
      "| id| name|   dept|salary|dense_rank|\n",
      "+---+-----+-------+------+----------+\n",
      "|  3|Priya|Finance| 60000|         1|\n",
      "|  2| Ravi|     HR| 52000|         2|\n",
      "|  1| Ravi|     HR| 50000|         3|\n",
      "|  6| John|     IT| 50000|         3|\n",
      "|  5| John|     IT| 48000|         4|\n",
      "|  4| John|     IT| 45000|         5|\n",
      "+---+-----+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    dense_rank().over(w).alias(\"dense_rank\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4342e1d9-b8b4-49ef-a54e-e80c50c7cc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+-------+\n",
      "| id| name|   dept|salary|lag_sal|\n",
      "+---+-----+-------+------+-------+\n",
      "|  3|Priya|Finance| 60000|   NULL|\n",
      "|  2| Ravi|     HR| 52000|  60000|\n",
      "|  1| Ravi|     HR| 50000|  52000|\n",
      "|  6| John|     IT| 50000|  50000|\n",
      "|  5| John|     IT| 48000|  50000|\n",
      "|  4| John|     IT| 45000|  48000|\n",
      "+---+-----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    lag(\"salary\",1).over(w).alias(\"lag_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5bda10-c791-4d55-820e-311216eb8bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+--------+\n",
      "| id| name|   dept|salary|lead_sal|\n",
      "+---+-----+-------+------+--------+\n",
      "|  3|Priya|Finance| 60000|   52000|\n",
      "|  2| Ravi|     HR| 52000|   50000|\n",
      "|  1| Ravi|     HR| 50000|   50000|\n",
      "|  6| John|     IT| 50000|   48000|\n",
      "|  5| John|     IT| 48000|   45000|\n",
      "|  4| John|     IT| 45000|    NULL|\n",
      "+---+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    lead(\"salary\").over(w).alias(\"lead_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "172bcc76-8231-4131-81c7-16451fa7b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+---------+\n",
      "| id| name|   dept|salary|first_sal|\n",
      "+---+-----+-------+------+---------+\n",
      "|  3|Priya|Finance| 60000|    60000|\n",
      "|  2| Ravi|     HR| 52000|    60000|\n",
      "|  1| Ravi|     HR| 50000|    60000|\n",
      "|  6| John|     IT| 50000|    60000|\n",
      "|  5| John|     IT| 48000|    60000|\n",
      "|  4| John|     IT| 45000|    60000|\n",
      "+---+-----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"*\",\n",
    "    first_value(\"salary\").over(w).alias(\"first_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1025b288-6ab9-4f38-903f-6b822f0b7af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+---------+\n",
      "| id| name|   dept|salary|first_sal|\n",
      "+---+-----+-------+------+---------+\n",
      "|  3|Priya|Finance| 60000|    60000|\n",
      "|  2| Ravi|     HR| 52000|    52000|\n",
      "|  1| Ravi|     HR| 50000|    50000|\n",
      "|  6| John|     IT| 50000|    50000|\n",
      "|  5| John|     IT| 48000|    48000|\n",
      "|  4| John|     IT| 45000|    45000|\n",
      "+---+-----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"*\",\n",
    "    last_value(\"salary\").over(w).alias(\"first_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc94ab78-7193-42c2-95ae-61b953432eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+---------+\n",
      "| id| name|   dept|salary|first_sal|\n",
      "+---+-----+-------+------+---------+\n",
      "|  3|Priya|Finance| 60000|    45000|\n",
      "|  2| Ravi|     HR| 52000|    45000|\n",
      "|  1| Ravi|     HR| 50000|    45000|\n",
      "|  6| John|     IT| 50000|    45000|\n",
      "|  5| John|     IT| 48000|    45000|\n",
      "|  4| John|     IT| 45000|    45000|\n",
      "+---+-----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc()).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    last_value(\"salary\").over(w).alias(\"first_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22820634-2728-4bf4-ada8-ecfd1da4f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+-------+------------------+-------+-------+------------------+\n",
      "| id| name|   dept|salary|sum_sal|           avg_sal|max_sal|min_sal|          sal_perc|\n",
      "+---+-----+-------+------+-------+------------------+-------+-------+------------------+\n",
      "|  3|Priya|Finance| 60000| 305000|50833.333333333336|  60000|  45000|19.672131147540984|\n",
      "|  2| Ravi|     HR| 52000| 305000|50833.333333333336|  60000|  45000| 17.04918032786885|\n",
      "|  1| Ravi|     HR| 50000| 305000|50833.333333333336|  60000|  45000| 16.39344262295082|\n",
      "|  6| John|     IT| 50000| 305000|50833.333333333336|  60000|  45000| 16.39344262295082|\n",
      "|  5| John|     IT| 48000| 305000|50833.333333333336|  60000|  45000|15.737704918032788|\n",
      "|  4| John|     IT| 45000| 305000|50833.333333333336|  60000|  45000|14.754098360655737|\n",
      "+---+-----+-------+------+-------+------------------+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc()).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df.select(\n",
    "    \"*\",\n",
    "    sum(\"salary\").over(w).alias(\"sum_sal\"),\n",
    "    avg(\"salary\").over(w).alias(\"avg_sal\"),\n",
    "    max(\"salary\").over(w).alias(\"max_sal\"),\n",
    "    min(\"salary\").over(w).alias(\"min_sal\"),\n",
    "    \n",
    "    (col(\"salary\")/ col(\"sum_sal\") *100).alias(\"sal_perc\") \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99fab11-5a4e-446a-ab7d-e2655ecce288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+-------+\n",
      "| id| name|   dept|salary|sum_sal|\n",
      "+---+-----+-------+------+-------+\n",
      "|  1| Ravi|     HR| 50000|  50000|\n",
      "|  2| Ravi|     HR| 52000| 102000|\n",
      "|  3|Priya|Finance| 60000| 162000|\n",
      "|  4| John|     IT| 45000| 207000|\n",
      "|  5| John|     IT| 48000| 255000|\n",
      "|  6| John|     IT| 50000| 305000|\n",
      "+---+-----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"id\").asc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df.select(\n",
    "    \"*\",\n",
    "    sum(\"salary\").over(w).alias(\"sum_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "207ae8e3-fe22-40e2-911e-904def15be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+------------------+\n",
      "| id| name|   dept|salary|           avg_sal|\n",
      "+---+-----+-------+------+------------------+\n",
      "|  1| Ravi|     HR| 50000|           50000.0|\n",
      "|  2| Ravi|     HR| 52000|           51000.0|\n",
      "|  3|Priya|Finance| 60000|           54000.0|\n",
      "|  4| John|     IT| 45000|           51750.0|\n",
      "|  5| John|     IT| 48000|           51000.0|\n",
      "|  6| John|     IT| 50000|50833.333333333336|\n",
      "+---+-----+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"id\").asc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df.select(\n",
    "    \"*\",\n",
    "    avg(\"salary\").over(w).alias(\"avg_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a3dad68-71cf-4d69-9cfb-f0faedd4a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+-------+\n",
      "| id| name|   dept|salary|sum_sal|\n",
      "+---+-----+-------+------+-------+\n",
      "|  1| Ravi|     HR| 50000|  50000|\n",
      "|  2| Ravi|     HR| 52000| 102000|\n",
      "|  3|Priya|Finance| 60000| 162000|\n",
      "|  4| John|     IT| 45000| 157000|\n",
      "|  5| John|     IT| 48000| 153000|\n",
      "|  6| John|     IT| 50000| 143000|\n",
      "+---+-----+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"id\").asc()).rowsBetween(-2, 0)\n",
    "df.select(\n",
    "    \"*\",\n",
    "    sum(\"salary\").over(w).alias(\"sum_sal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb5fd00-7544-4b22-972f-44c3dc253048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+--------+\n",
      "| id| name|   dept|salary|emp_rank|\n",
      "+---+-----+-------+------+--------+\n",
      "|  3|Priya|Finance| 60000|       1|\n",
      "|  2| Ravi|     HR| 52000|       2|\n",
      "|  1| Ravi|     HR| 50000|       3|\n",
      "|  6| John|     IT| 50000|       3|\n",
      "+---+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    dense_rank().over(w).alias(\"emp_rank\")\n",
    ").filter(col(\"emp_rank\") <=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d958c9-fa1c-485c-b26e-42a91b761b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+--------+\n",
      "| id|name|dept|salary|emp_rank|\n",
      "+---+----+----+------+--------+\n",
      "|  2|Ravi|  HR| 52000|       2|\n",
      "+---+----+----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.select(\n",
    "    \"*\",\n",
    "    dense_rank().over(w).alias(\"emp_rank\")\n",
    ").filter(col(\"emp_rank\") ==2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e18d7ae4-92fe-452c-a6bc-89d74affa291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b41ffe7a-58b7-4df5-b134-2ab03687ec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+\n",
      "| id| name|   dept|salary|\n",
      "+---+-----+-------+------+\n",
      "|  1| Ravi|     HR| 50000|\n",
      "|  2| Ravi|     HR| 52000|\n",
      "|  3|Priya|Finance| 60000|\n",
      "|  4| John|     IT| 45000|\n",
      "|  5| John|     IT| 48000|\n",
      "|  6| John|     IT| 50000|\n",
      "+---+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0df91429-1ea1-431a-b32e-8998bce25c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .csv(\"c:/data/emp_rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a048b9ff-7e02-42d6-bc8e-70340ded4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+\n",
      "| id| name|   dept|salary|\n",
      "+---+-----+-------+------+\n",
      "|  1| Ravi|     HR| 50000|\n",
      "|  2| Ravi|     HR| 52000|\n",
      "|  3|Priya|Finance| 60000|\n",
      "|  4| John|     IT| 45000|\n",
      "|  5| John|     IT| 48000|\n",
      "|  6| John|     IT| 50000|\n",
      "+---+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"c:/data/emp_rank\",header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a1088f9-a0ec-45e7-8562-851d38fd73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"c:/data/output_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aabd72f6-2163-4e63-abec-04187bc538c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+\n",
      "| id| name|   dept|salary|\n",
      "+---+-----+-------+------+\n",
      "|  1| Ravi|     HR| 50000|\n",
      "|  2| Ravi|     HR| 52000|\n",
      "|  3|Priya|Finance| 60000|\n",
      "|  4| John|     IT| 45000|\n",
      "|  5| John|     IT| 48000|\n",
      "|  6| John|     IT| 50000|\n",
      "+---+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"c:/data/output_parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2a9631a-014f-4deb-9ace-f61c515bf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .json(\"c:/data/output_json\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cedcf299-de33-4403-9630-1b626c09e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+------+\n",
      "|   dept| id| name|salary|\n",
      "+-------+---+-----+------+\n",
      "|     HR|  1| Ravi| 50000|\n",
      "|     HR|  2| Ravi| 52000|\n",
      "|Finance|  3|Priya| 60000|\n",
      "|     IT|  4| John| 45000|\n",
      "|     IT|  5| John| 48000|\n",
      "|     IT|  6| John| 50000|\n",
      "+-------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"c:/data/output_json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bb234f7-0475-4571-bf89-2e751797670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .orc(\"c:/data/output_orc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83f34b94-2cf0-4d4b-a109-7fcbaf877490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+------+\n",
      "|   dept| id| name|salary|\n",
      "+-------+---+-----+------+\n",
      "|     HR|  1| Ravi| 50000|\n",
      "|     HR|  2| Ravi| 52000|\n",
      "|Finance|  3|Priya| 60000|\n",
      "|     IT|  4| John| 45000|\n",
      "|     IT|  5| John| 48000|\n",
      "|     IT|  6| John| 50000|\n",
      "+-------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.orc(\"c:/data/output_orc\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a32efef-8692-4215-97bf-e0ae00d4400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.write \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"compression\", \"gzip\") \\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .csv(\"c:/data/output_csv_gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f2ea04a-5ced-46c8-9b05-f83690ec9c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+------+\n",
      "|   dept| id| name|salary|\n",
      "+-------+---+-----+------+\n",
      "|     HR|  1| Ravi| 50000|\n",
      "|     HR|  2| Ravi| 52000|\n",
      "|Finance|  3|Priya| 60000|\n",
      "|     IT|  4| John| 45000|\n",
      "|     IT|  5| John| 48000|\n",
      "|     IT|  6| John| 50000|\n",
      "+-------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "  .csv(\"c:/data/output_csv_gzip\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b7f4340-4d71-498e-9e2e-9d121393e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .partitionBy(\"dept\") \\\n",
    "  .parquet(\"c:/data/output_partitioned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b2b5507c-368e-4719-8321-ed74b17a89d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-----+\n",
      "|   dept| id|salary| name|\n",
      "+-------+---+------+-----+\n",
      "|     IT|  4| 45000| John|\n",
      "|     IT|  5| 48000| John|\n",
      "|     IT|  6| 50000| John|\n",
      "|Finance|  3| 60000|Priya|\n",
      "|     HR|  1| 50000| Ravi|\n",
      "|     HR|  2| 52000| Ravi|\n",
      "+-------+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"c:/data/output_partitioned\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82cd298b-269d-4960-beb3-cec1ca4e8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"OracleReadDemo\")\n",
    "        .master(\"local[*]\")\n",
    "        # Add JAR to Spark\n",
    "        .config(\"spark.jars\", r\"C:\\data\\ojdbc11-21.3.0.0.jar\")\n",
    "        .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4ce2f17-c275-4ca8-9d76-03c1467a1a0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o455.load.\n: java.lang.ClassNotFoundException: oracle.jdbc.OracleDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m conn = \u001b[33m\"\u001b[39m\u001b[33mjdbc:oracle:thin:@//localhost:1521/XE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m oracle_props = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33madmin123\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdriver\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33moracle.jdbc.OracleDriver\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      7\u001b[39m df_employees = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjdbc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdbtable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEMPLOYEES\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moracle_props\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m df_employees.show(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:318\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o455.load.\n: java.lang.ClassNotFoundException: oracle.jdbc.OracleDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "conn = \"jdbc:oracle:thin:@//localhost:1521/XE\"\n",
    "oracle_props = {\n",
    "    \"user\": \"HR\",\n",
    "    \"password\": \"admin123\",\n",
    "    \"driver\": \"oracle.jdbc.OracleDriver\"}\n",
    "\n",
    "df_employees = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", conn) \\\n",
    "    .option(\"dbtable\", \"EMPLOYEES\") \\\n",
    "    .options(**oracle_props) \\\n",
    "    .load()\n",
    "\n",
    "df_employees.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44c0d259-9146-42d3-942b-e920793bb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:oracle:thin:@//localhost:1521/XE\"\n",
    "\n",
    "df_emp_filtered.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"HR.dept_agg\") \\\n",
    "    .option(\"user\", \"HR\") \\\n",
    "    .option(\"password\", \"admin123\") \\\n",
    "    .option(\"driver\", \"oracle.jdbc.OracleDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc7e28b7-8029-4aee-9565-03b345682826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "| DEPARTMENT_NAME|     TOTAL_SALARY|\n",
      "+----------------+-----------------+\n",
      "|  Administration|  4400.0000000000|\n",
      "|      Accounting| 20308.0000000000|\n",
      "|       Executive| 63600.0000000000|\n",
      "|              IT| 33800.0000000000|\n",
      "|      Purchasing| 24900.0000000000|\n",
      "| Human Resources|  6500.0000000000|\n",
      "|Public Relations| 10000.0000000000|\n",
      "|        Shipping|156400.0000000000|\n",
      "|         Finance| 51608.0000000000|\n",
      "|           Sales|304500.0000000000|\n",
      "+----------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "url = \"jdbc:oracle:thin:@//localhost:1521/XE\"\n",
    "oracle_props = {\n",
    "    \"user\": \"HR\",\n",
    "    \"password\": \"admin123\",\n",
    "    \"driver\": \"oracle.jdbc.OracleDriver\"}\n",
    "\n",
    "\n",
    "df_emp_filtered = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"(select department_name, sum(salary) total_salary from employees e join\\\n",
    "    departments d on e.department_id = d.department_id group by department_name) T\") \\\n",
    "    .options(**oracle_props) \\\n",
    "    .load()\n",
    "\n",
    "df_emp_filtered.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d565bc5-8463-4c39-91a5-040292a85470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
